# ============================================================
# single_shot_question_generation_given_chunk.py
# ============================================================

import random
from dataclasses import dataclass, field
from typing import Any

from datasets import Dataset
from loguru import logger

from yourbench.utils.dataset_engine import custom_load_dataset, custom_save_dataset
from yourbench.utils.inference_engine import InferenceCall, run_inference

# Import the unified parsing function
from yourbench.utils.parsing_engine import parse_qa_pairs_from_response, shuffle_mcq
from yourbench.utils.prompts import (
    QUESTION_GENERATION_SYSTEM_PROMPT,
    QUESTION_GENERATION_SYSTEM_PROMPT_MULTI,
    QUESTION_GENERATION_USER_PROMPT,
)


@dataclass
class SingleHopQuestionRow:
    """
    Represents a single-hop question row derived from a single chunk of text.

    Attributes:
        chunk_id: A string identifier for the chunk from which this question was generated.
        chunk_text: The text content of the chunk.
        document_id: The identifier for the document containing the chunk.
        document_text: The full text of the document.
        additional_instructions: Any additional instructions provided for question generation.
        gem_id: The identifier for the GEMS (Generalized Entity Matching System) entry.
        gt_question: The ground truth question associated with the chunk.
        gt_answer: The ground truth answer associated with the chunk.
        question: The generated question based on the chunk.
        self_answer: The answer generated by the model for the question.
        choices: A list of answer choices if the question is multiple-choice.
        estimated_difficulty: An integer representing the estimated difficulty of the question (1-10).
        self_assessed_question_type: The type of question as assessed by the model (e.g., open-ended, multiple-choice).
        generating_model: The name of the model that generated the question.
        thought_process: The model's thought process or reasoning behind the question generation.
        raw_response: The raw response from the model, which may contain additional information.
        citations: A list of citations or references used in the question generation.
    """

    chunk_id: str
    chunk_text: str
    document_id: str
    document_text: str
    additional_instructions: str
    gem_id: str
    gt_question: str
    gt_answer: str
    question: str
    self_answer: str
    choices: list[str]
    estimated_difficulty: int
    self_assessed_question_type: str
    generating_model: str
    thought_process: str
    raw_response: str
    citations: list[str]


@dataclass
class ChunkSamplingConfig:
    mode: str = "all"
    value: float = 1.0
    random_seed: int = 42


@dataclass
class SingleShotQuestionGenerationConfig:
    run: bool = False
    source_subset: str = ""
    output_subset: str = ""
    additional_instructions: str = "Generate questions to test an undergraduate student"
    chunk_sampling: ChunkSamplingConfig = field(default_factory=ChunkSamplingConfig)
    question_type: str = "open-ended"


@dataclass
class DocumentRow:
    document_summary: str = "No summary available."
    document_filename: str = ""
    document_text: str = ""
    document_id: str = ""
    chunk_id: str = ""
    chunk_text: str = ""
    gem_id: str = ""
    gt_question: str = ""
    gt_answer: str = ""


def run(config: dict[str, Any]) -> None:
    """
    Executes the Single-Shot Question Generation stage of the pipeline.
    """
    stage_config = _load_stage_config(config)
    if not stage_config.run:
        logger.info("single_shot_question_generation stage is disabled. Skipping.")
        return

    dataset = custom_load_dataset(config=config, subset="answer_integrated")
    logger.info(f"Loaded answer-integrated subset with {len(dataset)} rows for Single-shot question generation.")

    inference_calls, call_index_mapping = _build_inference_calls(dataset, stage_config)
    if not inference_calls:
        logger.warning("No inference calls were created for single_shot_question_generation.")
        return

    responses_dict = _execute_inference(inference_calls, config)
    if not responses_dict:
        return

    question_dataset = _process_responses_and_build_dataset(responses_dict, call_index_mapping, stage_config)
    if question_dataset is None or len(question_dataset) == 0:
        logger.warning("No valid questions produced in single_shot_question_generation.")
        return

    custom_save_dataset(dataset=question_dataset, config=config, subset="single_shot_questions")
    logger.success("Single-shot question generation completed successfully.")


def _load_stage_config(config: dict[str, Any]) -> SingleShotQuestionGenerationConfig:
    """
    Extract the stage-specific configuration from the pipeline config.
    """
    pipeline_config = config.get("pipeline", {})
    stage_config_dict = pipeline_config.get("single_shot_question_generation_given_chunk", {})
    chunk_sampling_cfg = stage_config_dict.get("chunk_sampling", {})

    # For readability: if len(chunk_sampling_cfg) == 0
    if len(chunk_sampling_cfg) == 0:
        chunk_sampling = ChunkSamplingConfig()
    else:
        chunk_sampling = ChunkSamplingConfig(
            mode=chunk_sampling_cfg.get("mode", "all"),
            value=chunk_sampling_cfg.get("value", 1.0),
            random_seed=chunk_sampling_cfg.get("random_seed", 42),
        )

    return SingleShotQuestionGenerationConfig(
        run=stage_config_dict.get("run", False),
        source_subset=stage_config_dict.get("source_subset", ""),
        output_subset=stage_config_dict.get("output_subset", ""),
        additional_instructions=stage_config_dict.get("additional_instructions", "undergraduate"),
        chunk_sampling=chunk_sampling,
        question_type=stage_config_dict.get("question_type", "open-ended"),
    )


def _build_inference_calls(dataset, stage_config: SingleShotQuestionGenerationConfig):
    """
    Create the InferenceCall objects needed for single-shot question generation.
    Returns the list of calls and a parallel mapping of (row_index, doc_id, doc_text, chunk_id, chunk_text, gem_id, gt_question, gt_answer).
    """

    if stage_config.question_type == "multi-choice":
        system_prompt = QUESTION_GENERATION_SYSTEM_PROMPT_MULTI
    else:
        system_prompt = QUESTION_GENERATION_SYSTEM_PROMPT

    system_message = {"role": "system", "content": system_prompt}
    inference_calls = []
    call_index_mapping = []

    for row_index, row in enumerate(dataset):
        doc_row = DocumentRow(
            document_summary=row.get("document_summary", "No summary available."),
            document_filename=row.get("document_filename", ""),
            document_text=row.get("document_text", ""),
            document_id=row.get("document_id", ""),
            chunk_id=row.get("chunk_id", ""),
            chunk_text=row.get("chunk_text", ""),
            gem_id=row.get("gem_id", ""),
            gt_question=row.get("question", ""),
            gt_answer=row.get("answer", ""),
        )

        chunk_text = doc_row.chunk_text
        chunk_id = doc_row.chunk_id
        additional_instructions = """
The answer is given to you. You need to generate 10 questions. The answer to your generated questions should be:
<answer>
{answer}
</answer>
Make sure the questions are answerable by the given answer.
        """.strip().format(answer=doc_row.gt_answer)

        user_prompt_str = QUESTION_GENERATION_USER_PROMPT.format(
            title=doc_row.document_filename,
            document_summary=doc_row.document_summary,
            text_chunk=chunk_text,
            additional_instructions=additional_instructions,
        )
        user_message = {"role": "user", "content": user_prompt_str}

        inference_call = InferenceCall(messages=[system_message, user_message], tags=["single_shot_qa"])
        inference_calls.append(inference_call)
        call_index_mapping.append((row_index, doc_row.document_id, doc_row.document_text, chunk_id, chunk_text, doc_row.gem_id, doc_row.gt_question, doc_row.gt_answer))

    return inference_calls, call_index_mapping


def _execute_inference(inference_calls, config: dict[str, Any]):
    """
    Sends the prepared inference calls to the LLM(s). Returns a dict of responses.
    """
    logger.info(f"Sending {len(inference_calls)} calls to inference for single-shot question generation.")
    try:
        return run_inference(
            config=config,
            step_name="single_shot_question_generation",
            inference_calls=inference_calls,
        )
    except Exception as err:
        logger.error(f"Inference failed for single_shot_question_generation: {err}")
        return {}


def _process_responses_and_build_dataset(
    responses_dict: dict[str, list[str]],
    call_index_mapping: list[tuple],
    stage_config: SingleShotQuestionGenerationConfig,
) -> Dataset:
    """
    Take the LLM responses, parse them, and build a Hugging Face Dataset
    of single-shot question rows.
    """
    question_dataset_rows = []

    for model_name, model_responses in responses_dict.items():
        logger.info(f"Processing {len(model_responses)} responses from model: {model_name}")
        if len(model_responses) != len(call_index_mapping):
            logger.error(
                f"Model '{model_name}' returned {len(model_responses)} responses but expected {len(call_index_mapping)}. Mismatch."
            )

        for idx, raw_response in enumerate(model_responses):
            if idx >= len(call_index_mapping):
                break

            row_index, doc_id, doc_text, chunk_id, chunk_text, gem_id, gt_question, gt_answer = call_index_mapping[idx]
            qa_pairs = parse_qa_pairs_from_response(raw_response)

            # If parsing fails or returns nothing, still create a fallback row
            if not qa_pairs:
                logger.warning(
                    f"No parseable JSON found (or empty list) for row_index={row_index}, chunk_id={chunk_id}, model={model_name}. Creating fallback row."
                )
                continue

            # Otherwise, process each QA pair
            for pair in qa_pairs:
                try:
                    # Shuffle MCQ before extracting fields
                    pair = shuffle_mcq(pair)
                    # Safely extract data from pair
                    question_text = str(pair.get("question", "")).strip()
                    answer_text = str(pair.get("answer", "")).strip()
                    choices = pair.get("choices", [])
                    difficulty_val = _force_int_in_range(pair.get("estimated_difficulty", 5), 1, 10)
                    question_type = str(pair.get("question_type", "unknown"))
                    thought_process = str(pair.get("thought_process", ""))
                    citations = pair.get("citations", [])
                    if not isinstance(citations, list):
                        citations = []

                    if not question_text:
                        logger.debug(f"Empty question found; skipping this QA pair (row_index={row_index}).")
                        continue

                    # Build final row
                    question_row = SingleHopQuestionRow(
                        chunk_id=chunk_id,
                        chunk_text=chunk_text,
                        document_id=doc_id,
                        document_text=doc_text,
                        gem_id=gem_id,
                        gt_question=gt_question,
                        gt_answer=gt_answer,
                        # Use additional instructions from stage config
                        additional_instructions=stage_config.additional_instructions,
                        question=question_text,
                        self_answer=answer_text,
                        choices=choices,
                        estimated_difficulty=difficulty_val,
                        self_assessed_question_type=question_type,
                        generating_model=model_name,
                        thought_process=thought_process,
                        raw_response=raw_response,
                        citations=citations,
                    )
                    question_dataset_rows.append(question_row.__dict__)
                except Exception as e:
                    logger.error(f"Error processing QA pair for row_index={row_index}, chunk_id={chunk_id}: {e}")
                    continue

    if not question_dataset_rows:
        return None

    logger.info(f"Constructing final dataset with {len(question_dataset_rows)} single-hop questions.")
    column_names = list(question_dataset_rows[0].keys())
    final_data = {column: [row[column] for row in question_dataset_rows] for column in column_names}
    return Dataset.from_dict(final_data)


def _force_int_in_range(value: Any, min_val: int, max_val: int) -> int:
    """
    Convert a value to int and clamp it between min_val and max_val.
    """
    try:
        ivalue = int(value)
    except (ValueError, TypeError):
        ivalue = (min_val + max_val) // 2
    return max(min_val, min(ivalue, max_val))
